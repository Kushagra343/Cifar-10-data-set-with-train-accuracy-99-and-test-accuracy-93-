{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "from data_utils import *\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "import tensorflow as tf \n",
    "from keras import backend as keras\n",
    "import os\n",
    "config = tf.compat.v1.ConfigProto()\n",
    "# config.gpu_options.per_process_gpu_memory_fraction = 0.1\n",
    "config.gpu_options.allow_growth = True\n",
    "tf.compat.v1.keras.backend.set_session(tf.compat.v1.Session(config=config))\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CIFAR10 Training data shape: (50000, 32, 32, 3)\n",
      "CIFAR10 Training label shape (50000, 1)\n",
      "CIFAR10 Test data shape (10000, 32, 32, 3)\n",
      "CIFAR10 Test label shape (10000, 1)\n"
     ]
    }
   ],
   "source": [
    "# get data\n",
    "cifar10_data = CIFAR10Data()\n",
    "x_train, y_train, x_test, y_test = cifar10_data.get_data(subtract_mean=True)\n",
    "\n",
    "num_train = int(x_train.shape[0] * 0.9)\n",
    "num_val = x_train.shape[0] - num_train\n",
    "mask = list(range(num_train, num_train+num_val))\n",
    "x_val = x_train[mask]\n",
    "y_val = y_train[mask]\n",
    "\n",
    "mask = list(range(num_train))\n",
    "x_train = x_train[mask]\n",
    "y_train = y_train[mask]\n",
    "\n",
    "data = (x_train, y_train, x_val, y_val, x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test with resnet56\n",
    "resnet56 is inffered in the ResNet paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"resnet56\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 32, 32, 3)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 32, 32, 16)   432         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 32, 32, 16)   64          conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 32, 32, 16)   0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 32, 32, 16)   2304        activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 32, 32, 16)   64          conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 32, 32, 16)   0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 32, 32, 16)   2304        activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 32, 32, 16)   64          conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 32, 32, 16)   0           activation_1[0][0]               \n",
      "                                                                 batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 32, 32, 16)   0           add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 32, 32, 16)   2304        activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 32, 32, 16)   64          conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 32, 32, 16)   0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 32, 32, 16)   2304        activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 32, 32, 16)   64          conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_2 (Add)                     (None, 32, 32, 16)   0           activation_3[0][0]               \n",
      "                                                                 batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 32, 32, 16)   0           add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 32, 32, 16)   2304        activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 32, 32, 16)   64          conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 32, 32, 16)   0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 32, 32, 16)   2304        activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 32, 32, 16)   64          conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_3 (Add)                     (None, 32, 32, 16)   0           activation_5[0][0]               \n",
      "                                                                 batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 32, 32, 16)   0           add_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 32, 32, 16)   2304        activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 32, 32, 16)   64          conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 32, 32, 16)   0           batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, 32, 32, 16)   2304        activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 32, 32, 16)   64          conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_4 (Add)                     (None, 32, 32, 16)   0           activation_7[0][0]               \n",
      "                                                                 batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 32, 32, 16)   0           add_4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, 32, 32, 16)   2304        activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 32, 32, 16)   64          conv2d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, 32, 32, 16)   0           batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (None, 32, 32, 16)   2304        activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, 32, 32, 16)   64          conv2d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_5 (Add)                     (None, 32, 32, 16)   0           activation_9[0][0]               \n",
      "                                                                 batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, 32, 32, 16)   0           add_5[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (None, 32, 32, 16)   2304        activation_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, 32, 32, 16)   64          conv2d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (None, 32, 32, 16)   0           batch_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)              (None, 32, 32, 16)   2304        activation_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, 32, 32, 16)   64          conv2d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_6 (Add)                     (None, 32, 32, 16)   0           activation_11[0][0]              \n",
      "                                                                 batch_normalization_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_13 (Activation)      (None, 32, 32, 16)   0           add_6[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_14 (Conv2D)              (None, 32, 32, 16)   2304        activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNo (None, 32, 32, 16)   64          conv2d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_14 (Activation)      (None, 32, 32, 16)   0           batch_normalization_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_15 (Conv2D)              (None, 32, 32, 16)   2304        activation_14[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_15 (BatchNo (None, 32, 32, 16)   64          conv2d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_7 (Add)                     (None, 32, 32, 16)   0           activation_13[0][0]              \n",
      "                                                                 batch_normalization_15[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_15 (Activation)      (None, 32, 32, 16)   0           add_7[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_16 (Conv2D)              (None, 32, 32, 16)   2304        activation_15[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_16 (BatchNo (None, 32, 32, 16)   64          conv2d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_16 (Activation)      (None, 32, 32, 16)   0           batch_normalization_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_17 (Conv2D)              (None, 32, 32, 16)   2304        activation_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_17 (BatchNo (None, 32, 32, 16)   64          conv2d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_8 (Add)                     (None, 32, 32, 16)   0           activation_15[0][0]              \n",
      "                                                                 batch_normalization_17[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_17 (Activation)      (None, 32, 32, 16)   0           add_8[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_18 (Conv2D)              (None, 32, 32, 16)   2304        activation_17[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_18 (BatchNo (None, 32, 32, 16)   64          conv2d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_18 (Activation)      (None, 32, 32, 16)   0           batch_normalization_18[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_19 (Conv2D)              (None, 32, 32, 16)   2304        activation_18[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_19 (BatchNo (None, 32, 32, 16)   64          conv2d_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_9 (Add)                     (None, 32, 32, 16)   0           activation_17[0][0]              \n",
      "                                                                 batch_normalization_19[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_19 (Activation)      (None, 32, 32, 16)   0           add_9[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_21 (Conv2D)              (None, 16, 16, 32)   4608        activation_19[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_21 (BatchNo (None, 16, 16, 32)   128         conv2d_21[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_20 (Activation)      (None, 16, 16, 32)   0           batch_normalization_21[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_20 (Conv2D)              (None, 16, 16, 32)   512         activation_19[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_22 (Conv2D)              (None, 16, 16, 32)   9216        activation_20[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_20 (BatchNo (None, 16, 16, 32)   128         conv2d_20[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_22 (BatchNo (None, 16, 16, 32)   128         conv2d_22[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_10 (Add)                    (None, 16, 16, 32)   0           batch_normalization_20[0][0]     \n",
      "                                                                 batch_normalization_22[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_21 (Activation)      (None, 16, 16, 32)   0           add_10[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_23 (Conv2D)              (None, 16, 16, 32)   9216        activation_21[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_23 (BatchNo (None, 16, 16, 32)   128         conv2d_23[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_22 (Activation)      (None, 16, 16, 32)   0           batch_normalization_23[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_24 (Conv2D)              (None, 16, 16, 32)   9216        activation_22[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_24 (BatchNo (None, 16, 16, 32)   128         conv2d_24[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_11 (Add)                    (None, 16, 16, 32)   0           activation_21[0][0]              \n",
      "                                                                 batch_normalization_24[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_23 (Activation)      (None, 16, 16, 32)   0           add_11[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_25 (Conv2D)              (None, 16, 16, 32)   9216        activation_23[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_25 (BatchNo (None, 16, 16, 32)   128         conv2d_25[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_24 (Activation)      (None, 16, 16, 32)   0           batch_normalization_25[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_26 (Conv2D)              (None, 16, 16, 32)   9216        activation_24[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_26 (BatchNo (None, 16, 16, 32)   128         conv2d_26[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_12 (Add)                    (None, 16, 16, 32)   0           activation_23[0][0]              \n",
      "                                                                 batch_normalization_26[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_25 (Activation)      (None, 16, 16, 32)   0           add_12[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_27 (Conv2D)              (None, 16, 16, 32)   9216        activation_25[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_27 (BatchNo (None, 16, 16, 32)   128         conv2d_27[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_26 (Activation)      (None, 16, 16, 32)   0           batch_normalization_27[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_28 (Conv2D)              (None, 16, 16, 32)   9216        activation_26[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_28 (BatchNo (None, 16, 16, 32)   128         conv2d_28[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_13 (Add)                    (None, 16, 16, 32)   0           activation_25[0][0]              \n",
      "                                                                 batch_normalization_28[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_27 (Activation)      (None, 16, 16, 32)   0           add_13[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_29 (Conv2D)              (None, 16, 16, 32)   9216        activation_27[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_29 (BatchNo (None, 16, 16, 32)   128         conv2d_29[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_28 (Activation)      (None, 16, 16, 32)   0           batch_normalization_29[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_30 (Conv2D)              (None, 16, 16, 32)   9216        activation_28[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_30 (BatchNo (None, 16, 16, 32)   128         conv2d_30[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_14 (Add)                    (None, 16, 16, 32)   0           activation_27[0][0]              \n",
      "                                                                 batch_normalization_30[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_29 (Activation)      (None, 16, 16, 32)   0           add_14[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_31 (Conv2D)              (None, 16, 16, 32)   9216        activation_29[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_31 (BatchNo (None, 16, 16, 32)   128         conv2d_31[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_30 (Activation)      (None, 16, 16, 32)   0           batch_normalization_31[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_32 (Conv2D)              (None, 16, 16, 32)   9216        activation_30[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_32 (BatchNo (None, 16, 16, 32)   128         conv2d_32[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_15 (Add)                    (None, 16, 16, 32)   0           activation_29[0][0]              \n",
      "                                                                 batch_normalization_32[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_31 (Activation)      (None, 16, 16, 32)   0           add_15[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_33 (Conv2D)              (None, 16, 16, 32)   9216        activation_31[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_33 (BatchNo (None, 16, 16, 32)   128         conv2d_33[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_32 (Activation)      (None, 16, 16, 32)   0           batch_normalization_33[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_34 (Conv2D)              (None, 16, 16, 32)   9216        activation_32[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_34 (BatchNo (None, 16, 16, 32)   128         conv2d_34[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_16 (Add)                    (None, 16, 16, 32)   0           activation_31[0][0]              \n",
      "                                                                 batch_normalization_34[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_33 (Activation)      (None, 16, 16, 32)   0           add_16[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_35 (Conv2D)              (None, 16, 16, 32)   9216        activation_33[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_35 (BatchNo (None, 16, 16, 32)   128         conv2d_35[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_34 (Activation)      (None, 16, 16, 32)   0           batch_normalization_35[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_36 (Conv2D)              (None, 16, 16, 32)   9216        activation_34[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_36 (BatchNo (None, 16, 16, 32)   128         conv2d_36[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_17 (Add)                    (None, 16, 16, 32)   0           activation_33[0][0]              \n",
      "                                                                 batch_normalization_36[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_35 (Activation)      (None, 16, 16, 32)   0           add_17[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_37 (Conv2D)              (None, 16, 16, 32)   9216        activation_35[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_37 (BatchNo (None, 16, 16, 32)   128         conv2d_37[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_36 (Activation)      (None, 16, 16, 32)   0           batch_normalization_37[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_38 (Conv2D)              (None, 16, 16, 32)   9216        activation_36[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_38 (BatchNo (None, 16, 16, 32)   128         conv2d_38[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_18 (Add)                    (None, 16, 16, 32)   0           activation_35[0][0]              \n",
      "                                                                 batch_normalization_38[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_37 (Activation)      (None, 16, 16, 32)   0           add_18[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_40 (Conv2D)              (None, 8, 8, 64)     18432       activation_37[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_40 (BatchNo (None, 8, 8, 64)     256         conv2d_40[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_38 (Activation)      (None, 8, 8, 64)     0           batch_normalization_40[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_39 (Conv2D)              (None, 8, 8, 64)     2048        activation_37[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_41 (Conv2D)              (None, 8, 8, 64)     36864       activation_38[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_39 (BatchNo (None, 8, 8, 64)     256         conv2d_39[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_41 (BatchNo (None, 8, 8, 64)     256         conv2d_41[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_19 (Add)                    (None, 8, 8, 64)     0           batch_normalization_39[0][0]     \n",
      "                                                                 batch_normalization_41[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_39 (Activation)      (None, 8, 8, 64)     0           add_19[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_42 (Conv2D)              (None, 8, 8, 64)     36864       activation_39[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_42 (BatchNo (None, 8, 8, 64)     256         conv2d_42[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_40 (Activation)      (None, 8, 8, 64)     0           batch_normalization_42[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_43 (Conv2D)              (None, 8, 8, 64)     36864       activation_40[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_43 (BatchNo (None, 8, 8, 64)     256         conv2d_43[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_20 (Add)                    (None, 8, 8, 64)     0           activation_39[0][0]              \n",
      "                                                                 batch_normalization_43[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_41 (Activation)      (None, 8, 8, 64)     0           add_20[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_44 (Conv2D)              (None, 8, 8, 64)     36864       activation_41[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_44 (BatchNo (None, 8, 8, 64)     256         conv2d_44[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_42 (Activation)      (None, 8, 8, 64)     0           batch_normalization_44[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_45 (Conv2D)              (None, 8, 8, 64)     36864       activation_42[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_45 (BatchNo (None, 8, 8, 64)     256         conv2d_45[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_21 (Add)                    (None, 8, 8, 64)     0           activation_41[0][0]              \n",
      "                                                                 batch_normalization_45[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_43 (Activation)      (None, 8, 8, 64)     0           add_21[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_46 (Conv2D)              (None, 8, 8, 64)     36864       activation_43[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_46 (BatchNo (None, 8, 8, 64)     256         conv2d_46[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_44 (Activation)      (None, 8, 8, 64)     0           batch_normalization_46[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_47 (Conv2D)              (None, 8, 8, 64)     36864       activation_44[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_47 (BatchNo (None, 8, 8, 64)     256         conv2d_47[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_22 (Add)                    (None, 8, 8, 64)     0           activation_43[0][0]              \n",
      "                                                                 batch_normalization_47[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_45 (Activation)      (None, 8, 8, 64)     0           add_22[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_48 (Conv2D)              (None, 8, 8, 64)     36864       activation_45[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_48 (BatchNo (None, 8, 8, 64)     256         conv2d_48[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_46 (Activation)      (None, 8, 8, 64)     0           batch_normalization_48[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_49 (Conv2D)              (None, 8, 8, 64)     36864       activation_46[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_49 (BatchNo (None, 8, 8, 64)     256         conv2d_49[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_23 (Add)                    (None, 8, 8, 64)     0           activation_45[0][0]              \n",
      "                                                                 batch_normalization_49[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_47 (Activation)      (None, 8, 8, 64)     0           add_23[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_50 (Conv2D)              (None, 8, 8, 64)     36864       activation_47[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_50 (BatchNo (None, 8, 8, 64)     256         conv2d_50[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_48 (Activation)      (None, 8, 8, 64)     0           batch_normalization_50[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_51 (Conv2D)              (None, 8, 8, 64)     36864       activation_48[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_51 (BatchNo (None, 8, 8, 64)     256         conv2d_51[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_24 (Add)                    (None, 8, 8, 64)     0           activation_47[0][0]              \n",
      "                                                                 batch_normalization_51[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_49 (Activation)      (None, 8, 8, 64)     0           add_24[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_52 (Conv2D)              (None, 8, 8, 64)     36864       activation_49[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_52 (BatchNo (None, 8, 8, 64)     256         conv2d_52[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_50 (Activation)      (None, 8, 8, 64)     0           batch_normalization_52[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_53 (Conv2D)              (None, 8, 8, 64)     36864       activation_50[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_53 (BatchNo (None, 8, 8, 64)     256         conv2d_53[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_25 (Add)                    (None, 8, 8, 64)     0           activation_49[0][0]              \n",
      "                                                                 batch_normalization_53[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_51 (Activation)      (None, 8, 8, 64)     0           add_25[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_54 (Conv2D)              (None, 8, 8, 64)     36864       activation_51[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_54 (BatchNo (None, 8, 8, 64)     256         conv2d_54[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_52 (Activation)      (None, 8, 8, 64)     0           batch_normalization_54[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_55 (Conv2D)              (None, 8, 8, 64)     36864       activation_52[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_55 (BatchNo (None, 8, 8, 64)     256         conv2d_55[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_26 (Add)                    (None, 8, 8, 64)     0           activation_51[0][0]              \n",
      "                                                                 batch_normalization_55[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_53 (Activation)      (None, 8, 8, 64)     0           add_26[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_56 (Conv2D)              (None, 8, 8, 64)     36864       activation_53[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_56 (BatchNo (None, 8, 8, 64)     256         conv2d_56[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_54 (Activation)      (None, 8, 8, 64)     0           batch_normalization_56[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_57 (Conv2D)              (None, 8, 8, 64)     36864       activation_54[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_57 (BatchNo (None, 8, 8, 64)     256         conv2d_57[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_27 (Add)                    (None, 8, 8, 64)     0           activation_53[0][0]              \n",
      "                                                                 batch_normalization_57[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_55 (Activation)      (None, 8, 8, 64)     0           add_27[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_1 (AveragePoo (None, 1, 1, 64)     0           activation_55[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 64)           0           average_pooling2d_1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 10)           650         flatten_1[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 860,026\n",
      "Trainable params: 855,770\n",
      "Non-trainable params: 4,256\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from classifiers.ResNet import ResNet56ForCIFAR10\n",
    "from keras import losses\n",
    "from keras import optimizers\n",
    "\n",
    "weight_decay = 1e-4\n",
    "lr = 1e-1\n",
    "num_classes = 10\n",
    "resnet56 = ResNet56ForCIFAR10(input_shape=(32, 32, 3), classes=num_classes, weight_decay=weight_decay)\n",
    "opt = optimizers.SGD(lr=lr, momentum=0.9, nesterov=False)\n",
    "resnet56.compile(optimizer=opt,\n",
    "                 loss=losses.categorical_crossentropy,\n",
    "                 metrics=['accuracy'])\n",
    "resnet56.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train with data augmentation\n",
      "Epoch 1/200\n",
      "Nitin Face X Assignment Last Date 24-feb-2019 new lr:1.00e-01\n",
      "352/352 [==============================] - 110s 312ms/step - loss: 2.4521 - accuracy: 0.2644 - val_loss: 5.7187 - val_accuracy: 0.1266\n",
      "Epoch 2/200\n",
      "Nitin Face X Assignment Last Date 24-feb-2019 new lr:1.00e-01\n",
      "352/352 [==============================] - 93s 264ms/step - loss: 1.8431 - accuracy: 0.4591 - val_loss: 2.5335 - val_accuracy: 0.3432\n",
      "Epoch 3/200\n",
      "Nitin Face X Assignment Last Date 24-feb-2019 new lr:1.00e-01\n",
      "352/352 [==============================] - 93s 266ms/step - loss: 1.4898 - accuracy: 0.5895 - val_loss: 1.7901 - val_accuracy: 0.5278\n",
      "Epoch 4/200\n",
      "Nitin Face X Assignment Last Date 24-feb-2019 new lr:1.00e-01\n",
      "352/352 [==============================] - 93s 265ms/step - loss: 1.2698 - accuracy: 0.6623 - val_loss: 2.3025 - val_accuracy: 0.4908\n",
      "Epoch 5/200\n",
      "Nitin Face X Assignment Last Date 24-feb-2019 new lr:1.00e-01\n",
      "352/352 [==============================] - 93s 265ms/step - loss: 1.1060 - accuracy: 0.7181 - val_loss: 1.2491 - val_accuracy: 0.6772\n",
      "Epoch 6/200\n",
      "Nitin Face X Assignment Last Date 24-feb-2019 new lr:1.00e-01\n",
      "352/352 [==============================] - 93s 265ms/step - loss: 0.9920 - accuracy: 0.7556 - val_loss: 1.0524 - val_accuracy: 0.7386\n",
      "Epoch 7/200\n",
      "Nitin Face X Assignment Last Date 24-feb-2019 new lr:1.00e-01\n",
      "352/352 [==============================] - 93s 265ms/step - loss: 0.9187 - accuracy: 0.7752 - val_loss: 0.9453 - val_accuracy: 0.7666\n",
      "Epoch 8/200\n",
      "Nitin Face X Assignment Last Date 24-feb-2019 new lr:1.00e-01\n",
      "352/352 [==============================] - 93s 265ms/step - loss: 0.8541 - accuracy: 0.7965 - val_loss: 1.0498 - val_accuracy: 0.7276\n",
      "Epoch 9/200\n",
      "Nitin Face X Assignment Last Date 24-feb-2019 new lr:1.00e-01\n",
      "352/352 [==============================] - 93s 265ms/step - loss: 0.8068 - accuracy: 0.8076 - val_loss: 1.1044 - val_accuracy: 0.7088\n",
      "Epoch 10/200\n",
      "Nitin Face X Assignment Last Date 24-feb-2019 new lr:1.00e-01\n",
      "352/352 [==============================] - 93s 266ms/step - loss: 0.7666 - accuracy: 0.8179 - val_loss: 0.9142 - val_accuracy: 0.7824\n",
      "Epoch 11/200\n",
      "Nitin Face X Assignment Last Date 24-feb-2019 new lr:1.00e-01\n",
      "352/352 [==============================] - 93s 265ms/step - loss: 0.7398 - accuracy: 0.8266 - val_loss: 0.9111 - val_accuracy: 0.7700\n",
      "Epoch 12/200\n",
      "Nitin Face X Assignment Last Date 24-feb-2019 new lr:1.00e-01\n",
      "352/352 [==============================] - 94s 266ms/step - loss: 0.7202 - accuracy: 0.8318 - val_loss: 0.7662 - val_accuracy: 0.8182\n",
      "Epoch 13/200\n",
      "Nitin Face X Assignment Last Date 24-feb-2019 new lr:1.00e-01\n",
      "352/352 [==============================] - 93s 265ms/step - loss: 0.6852 - accuracy: 0.8408 - val_loss: 0.9216 - val_accuracy: 0.7664\n",
      "Epoch 14/200\n",
      "Nitin Face X Assignment Last Date 24-feb-2019 new lr:1.00e-01\n",
      "352/352 [==============================] - 94s 266ms/step - loss: 0.6645 - accuracy: 0.8473 - val_loss: 0.7893 - val_accuracy: 0.8038\n",
      "Epoch 15/200\n",
      "Nitin Face X Assignment Last Date 24-feb-2019 new lr:1.00e-01\n",
      "352/352 [==============================] - 93s 266ms/step - loss: 0.6508 - accuracy: 0.8533 - val_loss: 1.0059 - val_accuracy: 0.7624\n",
      "Epoch 16/200\n",
      "Nitin Face X Assignment Last Date 24-feb-2019 new lr:1.00e-01\n",
      "352/352 [==============================] - 93s 266ms/step - loss: 0.6428 - accuracy: 0.8540 - val_loss: 0.7123 - val_accuracy: 0.8344\n",
      "Epoch 17/200\n",
      "Nitin Face X Assignment Last Date 24-feb-2019 new lr:1.00e-01\n",
      "352/352 [==============================] - 93s 266ms/step - loss: 0.6205 - accuracy: 0.8623 - val_loss: 0.8833 - val_accuracy: 0.7820\n",
      "Epoch 18/200\n",
      "Nitin Face X Assignment Last Date 24-feb-2019 new lr:1.00e-01\n",
      "352/352 [==============================] - 94s 267ms/step - loss: 0.6031 - accuracy: 0.8672 - val_loss: 0.7678 - val_accuracy: 0.8118\n",
      "Epoch 19/200\n",
      "Nitin Face X Assignment Last Date 24-feb-2019 new lr:1.00e-01\n",
      "352/352 [==============================] - 94s 268ms/step - loss: 0.5958 - accuracy: 0.8697 - val_loss: 0.8775 - val_accuracy: 0.8016\n",
      "Epoch 20/200\n",
      "Nitin Face X Assignment Last Date 24-feb-2019 new lr:1.00e-01\n",
      "352/352 [==============================] - 94s 268ms/step - loss: 0.5841 - accuracy: 0.8747 - val_loss: 0.8188 - val_accuracy: 0.8058\n",
      "Epoch 21/200\n",
      "Nitin Face X Assignment Last Date 24-feb-2019 new lr:1.00e-01\n",
      "352/352 [==============================] - 94s 268ms/step - loss: 0.5738 - accuracy: 0.8783 - val_loss: 0.8385 - val_accuracy: 0.8126\n",
      "Epoch 22/200\n",
      "Nitin Face X Assignment Last Date 24-feb-2019 new lr:1.00e-01\n",
      "352/352 [==============================] - 94s 268ms/step - loss: 0.5678 - accuracy: 0.8810 - val_loss: 1.0336 - val_accuracy: 0.7506\n",
      "Epoch 23/200\n",
      "Nitin Face X Assignment Last Date 24-feb-2019 new lr:1.00e-01\n",
      "352/352 [==============================] - 94s 268ms/step - loss: 0.5661 - accuracy: 0.8824 - val_loss: 0.7657 - val_accuracy: 0.8146\n",
      "Epoch 24/200\n",
      "Nitin Face X Assignment Last Date 24-feb-2019 new lr:1.00e-01\n",
      "352/352 [==============================] - 94s 268ms/step - loss: 0.5548 - accuracy: 0.8852 - val_loss: 0.8026 - val_accuracy: 0.8134\n",
      "Epoch 25/200\n",
      "Nitin Face X Assignment Last Date 24-feb-2019 new lr:1.00e-01\n",
      "352/352 [==============================] - 95s 268ms/step - loss: 0.5451 - accuracy: 0.8879 - val_loss: 0.7976 - val_accuracy: 0.8174\n",
      "Epoch 26/200\n",
      "Nitin Face X Assignment Last Date 24-feb-2019 new lr:1.00e-01\n",
      "352/352 [==============================] - 94s 268ms/step - loss: 0.5436 - accuracy: 0.8905 - val_loss: 0.7186 - val_accuracy: 0.8376\n",
      "Epoch 27/200\n",
      "Nitin Face X Assignment Last Date 24-feb-2019 new lr:1.00e-01\n",
      "352/352 [==============================] - 94s 268ms/step - loss: 0.5367 - accuracy: 0.8921 - val_loss: 1.2207 - val_accuracy: 0.7248\n",
      "Epoch 28/200\n",
      "Nitin Face X Assignment Last Date 24-feb-2019 new lr:1.00e-01\n",
      "352/352 [==============================] - 94s 268ms/step - loss: 0.5334 - accuracy: 0.8957 - val_loss: 0.7117 - val_accuracy: 0.8390\n",
      "Epoch 29/200\n",
      "Nitin Face X Assignment Last Date 24-feb-2019 new lr:1.00e-01\n",
      "352/352 [==============================] - 94s 268ms/step - loss: 0.5323 - accuracy: 0.8956 - val_loss: 0.6638 - val_accuracy: 0.8586\n",
      "Epoch 30/200\n",
      "Nitin Face X Assignment Last Date 24-feb-2019 new lr:1.00e-01\n",
      "352/352 [==============================] - 95s 268ms/step - loss: 0.5213 - accuracy: 0.8992 - val_loss: 0.8454 - val_accuracy: 0.8134\n",
      "Epoch 31/200\n",
      "Nitin Face X Assignment Last Date 24-feb-2019 new lr:1.00e-01\n",
      "352/352 [==============================] - 94s 268ms/step - loss: 0.5222 - accuracy: 0.8993 - val_loss: 0.7707 - val_accuracy: 0.8340\n",
      "Epoch 32/200\n",
      "Nitin Face X Assignment Last Date 24-feb-2019 new lr:1.00e-01\n",
      "352/352 [==============================] - 94s 268ms/step - loss: 0.5127 - accuracy: 0.9028 - val_loss: 0.9601 - val_accuracy: 0.7884\n",
      "Epoch 33/200\n",
      "Nitin Face X Assignment Last Date 24-feb-2019 new lr:1.00e-01\n",
      "352/352 [==============================] - 94s 268ms/step - loss: 0.5161 - accuracy: 0.9026 - val_loss: 0.7199 - val_accuracy: 0.8414\n",
      "Epoch 34/200\n",
      "Nitin Face X Assignment Last Date 24-feb-2019 new lr:1.00e-01\n",
      "352/352 [==============================] - 94s 268ms/step - loss: 0.5077 - accuracy: 0.9076 - val_loss: 0.7169 - val_accuracy: 0.8502\n",
      "Epoch 35/200\n",
      "Nitin Face X Assignment Last Date 24-feb-2019 new lr:1.00e-01\n",
      "352/352 [==============================] - 94s 268ms/step - loss: 0.5094 - accuracy: 0.9052 - val_loss: 0.9089 - val_accuracy: 0.7834\n",
      "Epoch 36/200\n",
      "Nitin Face X Assignment Last Date 24-feb-2019 new lr:1.00e-01\n",
      "352/352 [==============================] - 94s 268ms/step - loss: 0.5032 - accuracy: 0.9087 - val_loss: 0.7831 - val_accuracy: 0.8296\n",
      "Epoch 37/200\n",
      "Nitin Face X Assignment Last Date 24-feb-2019 new lr:1.00e-01\n",
      "352/352 [==============================] - 94s 268ms/step - loss: 0.5063 - accuracy: 0.9058 - val_loss: 0.7414 - val_accuracy: 0.8432\n",
      "Epoch 38/200\n",
      "Nitin Face X Assignment Last Date 24-feb-2019 new lr:1.00e-01\n",
      "352/352 [==============================] - 94s 268ms/step - loss: 0.4971 - accuracy: 0.9106 - val_loss: 0.7817 - val_accuracy: 0.8238\n",
      "Epoch 39/200\n",
      "Nitin Face X Assignment Last Date 24-feb-2019 new lr:1.00e-01\n",
      "352/352 [==============================] - 94s 268ms/step - loss: 0.4951 - accuracy: 0.9126 - val_loss: 0.7665 - val_accuracy: 0.8334\n",
      "Epoch 40/200\n",
      "Nitin Face X Assignment Last Date 24-feb-2019 new lr:1.00e-01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "352/352 [==============================] - 93s 265ms/step - loss: 0.4991 - accuracy: 0.9124 - val_loss: 0.8278 - val_accuracy: 0.8108\n",
      "Epoch 41/200\n",
      "Nitin Face X Assignment Last Date 24-feb-2019 new lr:1.00e-01\n",
      "352/352 [==============================] - 93s 265ms/step - loss: 0.4942 - accuracy: 0.9138 - val_loss: 0.8087 - val_accuracy: 0.8170\n",
      "Epoch 42/200\n",
      "Nitin Face X Assignment Last Date 24-feb-2019 new lr:1.00e-01\n",
      "352/352 [==============================] - 93s 265ms/step - loss: 0.4956 - accuracy: 0.9126 - val_loss: 0.8789 - val_accuracy: 0.8130\n",
      "Epoch 43/200\n",
      "Nitin Face X Assignment Last Date 24-feb-2019 new lr:1.00e-01\n",
      "352/352 [==============================] - 93s 265ms/step - loss: 0.4892 - accuracy: 0.9166 - val_loss: 0.8366 - val_accuracy: 0.8162\n",
      "Epoch 44/200\n",
      "Nitin Face X Assignment Last Date 24-feb-2019 new lr:1.00e-01\n",
      "352/352 [==============================] - 93s 265ms/step - loss: 0.4960 - accuracy: 0.9144 - val_loss: 0.8630 - val_accuracy: 0.8116\n",
      "Epoch 45/200\n",
      "Nitin Face X Assignment Last Date 24-feb-2019 new lr:1.00e-01\n",
      "352/352 [==============================] - 93s 265ms/step - loss: 0.4899 - accuracy: 0.9154 - val_loss: 0.6614 - val_accuracy: 0.8630\n",
      "Epoch 46/200\n",
      "Nitin Face X Assignment Last Date 24-feb-2019 new lr:1.00e-01\n",
      "352/352 [==============================] - 93s 265ms/step - loss: 0.4980 - accuracy: 0.9140 - val_loss: 0.8863 - val_accuracy: 0.8198\n",
      "Epoch 47/200\n",
      "Nitin Face X Assignment Last Date 24-feb-2019 new lr:1.00e-01\n",
      "352/352 [==============================] - 93s 265ms/step - loss: 0.4906 - accuracy: 0.9166 - val_loss: 0.8058 - val_accuracy: 0.8236\n",
      "Epoch 48/200\n",
      "Nitin Face X Assignment Last Date 24-feb-2019 new lr:1.00e-01\n",
      "352/352 [==============================] - 93s 265ms/step - loss: 0.4865 - accuracy: 0.9177 - val_loss: 1.0145 - val_accuracy: 0.7994\n",
      "Epoch 49/200\n",
      "Nitin Face X Assignment Last Date 24-feb-2019 new lr:1.00e-01\n",
      "352/352 [==============================] - 93s 265ms/step - loss: 0.4855 - accuracy: 0.9190 - val_loss: 1.0317 - val_accuracy: 0.7680\n",
      "Epoch 50/200\n",
      "Nitin Face X Assignment Last Date 24-feb-2019 new lr:1.00e-01\n",
      "352/352 [==============================] - 93s 265ms/step - loss: 0.4899 - accuracy: 0.9188 - val_loss: 0.8099 - val_accuracy: 0.8156\n",
      "Epoch 51/200\n",
      "Nitin Face X Assignment Last Date 24-feb-2019 new lr:1.00e-01\n",
      "352/352 [==============================] - 93s 265ms/step - loss: 0.4802 - accuracy: 0.9216 - val_loss: 0.7177 - val_accuracy: 0.8542\n",
      "Epoch 52/200\n",
      "Nitin Face X Assignment Last Date 24-feb-2019 new lr:1.00e-01\n",
      "352/352 [==============================] - 93s 265ms/step - loss: 0.4853 - accuracy: 0.9198 - val_loss: 0.7779 - val_accuracy: 0.8408\n",
      "Epoch 53/200\n",
      "Nitin Face X Assignment Last Date 24-feb-2019 new lr:1.00e-01\n",
      "352/352 [==============================] - 93s 265ms/step - loss: 0.4808 - accuracy: 0.9227 - val_loss: 0.8378 - val_accuracy: 0.8242\n",
      "Epoch 54/200\n",
      "Nitin Face X Assignment Last Date 24-feb-2019 new lr:1.00e-01\n",
      "352/352 [==============================] - 94s 266ms/step - loss: 0.4772 - accuracy: 0.9235 - val_loss: 0.9060 - val_accuracy: 0.8154\n",
      "Epoch 55/200\n",
      "Nitin Face X Assignment Last Date 24-feb-2019 new lr:1.00e-01\n",
      "352/352 [==============================] - 93s 265ms/step - loss: 0.4787 - accuracy: 0.9228 - val_loss: 0.6816 - val_accuracy: 0.8548\n",
      "Epoch 56/200\n",
      "Nitin Face X Assignment Last Date 24-feb-2019 new lr:1.00e-01\n",
      "352/352 [==============================] - 93s 265ms/step - loss: 0.4742 - accuracy: 0.9243 - val_loss: 0.8590 - val_accuracy: 0.8188\n",
      "Epoch 57/200\n",
      "Nitin Face X Assignment Last Date 24-feb-2019 new lr:1.00e-01\n",
      "352/352 [==============================] - 93s 265ms/step - loss: 0.4783 - accuracy: 0.9238 - val_loss: 0.8562 - val_accuracy: 0.8126\n",
      "Epoch 58/200\n",
      "Nitin Face X Assignment Last Date 24-feb-2019 new lr:1.00e-01\n",
      "352/352 [==============================] - 94s 266ms/step - loss: 0.4804 - accuracy: 0.9230 - val_loss: 0.7873 - val_accuracy: 0.8342\n",
      "Epoch 59/200\n",
      "Nitin Face X Assignment Last Date 24-feb-2019 new lr:1.00e-01\n",
      "352/352 [==============================] - 94s 266ms/step - loss: 0.4779 - accuracy: 0.9234 - val_loss: 0.6458 - val_accuracy: 0.8756\n",
      "Epoch 60/200\n",
      "Nitin Face X Assignment Last Date 24-feb-2019 new lr:1.00e-01\n",
      "352/352 [==============================] - 93s 266ms/step - loss: 0.4707 - accuracy: 0.9269 - val_loss: 0.9160 - val_accuracy: 0.8084\n",
      "Epoch 61/200\n",
      "Nitin Face X Assignment Last Date 24-feb-2019 new lr:1.00e-01\n",
      "352/352 [==============================] - 94s 266ms/step - loss: 0.4713 - accuracy: 0.9260 - val_loss: 0.6954 - val_accuracy: 0.8634\n",
      "Epoch 62/200\n",
      "Nitin Face X Assignment Last Date 24-feb-2019 new lr:1.00e-01\n",
      "352/352 [==============================] - 94s 266ms/step - loss: 0.4707 - accuracy: 0.9256 - val_loss: 0.7332 - val_accuracy: 0.8504\n",
      "Epoch 63/200\n",
      "Nitin Face X Assignment Last Date 24-feb-2019 new lr:1.00e-01\n",
      "352/352 [==============================] - 94s 266ms/step - loss: 0.4698 - accuracy: 0.9274 - val_loss: 0.8568 - val_accuracy: 0.8294\n",
      "Epoch 64/200\n",
      "Nitin Face X Assignment Last Date 24-feb-2019 new lr:1.00e-01\n",
      "352/352 [==============================] - 94s 266ms/step - loss: 0.4737 - accuracy: 0.9251 - val_loss: 0.8347 - val_accuracy: 0.8354\n",
      "Epoch 65/200\n",
      "Nitin Face X Assignment Last Date 24-feb-2019 new lr:1.00e-01\n",
      "352/352 [==============================] - 93s 266ms/step - loss: 0.4664 - accuracy: 0.9283 - val_loss: 0.6110 - val_accuracy: 0.8810\n",
      "Epoch 66/200\n",
      "Nitin Face X Assignment Last Date 24-feb-2019 new lr:1.00e-01\n",
      "352/352 [==============================] - 93s 266ms/step - loss: 0.4706 - accuracy: 0.9282 - val_loss: 0.6987 - val_accuracy: 0.8634\n",
      "Epoch 67/200\n",
      "Nitin Face X Assignment Last Date 24-feb-2019 new lr:1.00e-01\n",
      "352/352 [==============================] - 94s 266ms/step - loss: 0.4727 - accuracy: 0.9277 - val_loss: 1.0664 - val_accuracy: 0.7864\n",
      "Epoch 68/200\n",
      "Nitin Face X Assignment Last Date 24-feb-2019 new lr:1.00e-01\n",
      "352/352 [==============================] - 94s 266ms/step - loss: 0.4698 - accuracy: 0.9283 - val_loss: 0.8328 - val_accuracy: 0.8242\n",
      "Epoch 69/200\n",
      "Nitin Face X Assignment Last Date 24-feb-2019 new lr:1.00e-01\n",
      "352/352 [==============================] - 94s 266ms/step - loss: 0.4601 - accuracy: 0.9328 - val_loss: 0.7539 - val_accuracy: 0.8422\n",
      "Epoch 70/200\n",
      "Nitin Face X Assignment Last Date 24-feb-2019 new lr:1.00e-01\n",
      "352/352 [==============================] - 94s 266ms/step - loss: 0.4647 - accuracy: 0.9309 - val_loss: 0.7537 - val_accuracy: 0.8530\n",
      "Epoch 71/200\n",
      "Nitin Face X Assignment Last Date 24-feb-2019 new lr:1.00e-01\n",
      "352/352 [==============================] - 94s 267ms/step - loss: 0.4665 - accuracy: 0.9285 - val_loss: 0.8113 - val_accuracy: 0.8382\n",
      "Epoch 72/200\n",
      "Nitin Face X Assignment Last Date 24-feb-2019 new lr:1.00e-01\n",
      "352/352 [==============================] - 94s 266ms/step - loss: 0.4628 - accuracy: 0.9304 - val_loss: 0.7689 - val_accuracy: 0.8494\n",
      "Epoch 73/200\n",
      "Nitin Face X Assignment Last Date 24-feb-2019 new lr:1.00e-01\n",
      "352/352 [==============================] - 94s 266ms/step - loss: 0.4675 - accuracy: 0.9301 - val_loss: 0.7625 - val_accuracy: 0.8372\n",
      "Epoch 74/200\n",
      "Nitin Face X Assignment Last Date 24-feb-2019 new lr:1.00e-01\n",
      "352/352 [==============================] - 94s 266ms/step - loss: 0.4645 - accuracy: 0.9302 - val_loss: 0.8040 - val_accuracy: 0.8366\n",
      "Epoch 75/200\n",
      "Nitin Face X Assignment Last Date 24-feb-2019 new lr:1.00e-01\n",
      "352/352 [==============================] - 94s 266ms/step - loss: 0.4669 - accuracy: 0.9305 - val_loss: 0.7185 - val_accuracy: 0.8600\n",
      "Epoch 76/200\n",
      "Nitin Face X Assignment Last Date 24-feb-2019 new lr:1.00e-01\n",
      "352/352 [==============================] - 94s 266ms/step - loss: 0.4647 - accuracy: 0.9295 - val_loss: 0.8556 - val_accuracy: 0.8176\n",
      "Epoch 77/200\n",
      "Nitin Face X Assignment Last Date 24-feb-2019 new lr:1.00e-01\n",
      "352/352 [==============================] - 94s 266ms/step - loss: 0.4661 - accuracy: 0.9306 - val_loss: 0.7319 - val_accuracy: 0.8556\n",
      "Epoch 78/200\n",
      "Nitin Face X Assignment Last Date 24-feb-2019 new lr:1.00e-01\n",
      "352/352 [==============================] - 94s 266ms/step - loss: 0.4591 - accuracy: 0.9327 - val_loss: 0.8598 - val_accuracy: 0.8260\n",
      "Epoch 79/200\n",
      "Nitin Face X Assignment Last Date 24-feb-2019 new lr:1.00e-01\n",
      "352/352 [==============================] - 93s 265ms/step - loss: 0.4693 - accuracy: 0.9296 - val_loss: 0.8087 - val_accuracy: 0.8442\n",
      "Epoch 80/200\n",
      "Nitin Face X Assignment Last Date 24-feb-2019 new lr:1.00e-01\n",
      "352/352 [==============================] - 93s 265ms/step - loss: 0.4597 - accuracy: 0.9330 - val_loss: 0.8302 - val_accuracy: 0.8352\n",
      "Epoch 81/200\n",
      "Nitin Face X Assignment Last Date 24-feb-2019 new lr:1.00e-01\n",
      "352/352 [==============================] - 93s 265ms/step - loss: 0.4575 - accuracy: 0.9327 - val_loss: 0.7654 - val_accuracy: 0.8354\n",
      "Epoch 82/200\n",
      "Nitin Face X Assignment Last Date 24-feb-2019 new lr:1.00e-01\n",
      "352/352 [==============================] - 93s 265ms/step - loss: 0.4554 - accuracy: 0.9352 - val_loss: 0.8634 - val_accuracy: 0.8320\n",
      "Epoch 83/200\n",
      "Nitin Face X Assignment Last Date 24-feb-2019 new lr:1.00e-01\n",
      "352/352 [==============================] - 93s 265ms/step - loss: 0.4594 - accuracy: 0.9330 - val_loss: 0.8979 - val_accuracy: 0.8134\n",
      "Epoch 84/200\n",
      "Nitin Face X Assignment Last Date 24-feb-2019 new lr:1.00e-01\n",
      "352/352 [==============================] - 93s 265ms/step - loss: 0.4648 - accuracy: 0.9312 - val_loss: 0.6942 - val_accuracy: 0.8660\n",
      "Epoch 85/200\n",
      "Nitin Face X Assignment Last Date 24-feb-2019 new lr:1.00e-01\n",
      "352/352 [==============================] - 93s 266ms/step - loss: 0.4489 - accuracy: 0.9377 - val_loss: 0.8327 - val_accuracy: 0.8336\n",
      "Epoch 86/200\n",
      "Nitin Face X Assignment Last Date 24-feb-2019 new lr:1.00e-01\n",
      "352/352 [==============================] - 93s 265ms/step - loss: 0.4527 - accuracy: 0.9357 - val_loss: 0.6852 - val_accuracy: 0.8712\n",
      "Epoch 87/200\n",
      "Nitin Face X Assignment Last Date 24-feb-2019 new lr:1.00e-01\n",
      "352/352 [==============================] - 93s 265ms/step - loss: 0.4607 - accuracy: 0.9324 - val_loss: 0.6974 - val_accuracy: 0.8650\n",
      "Epoch 88/200\n",
      "Nitin Face X Assignment Last Date 24-feb-2019 new lr:1.00e-01\n",
      "352/352 [==============================] - 93s 265ms/step - loss: 0.4553 - accuracy: 0.9350 - val_loss: 0.7300 - val_accuracy: 0.8558\n",
      "Epoch 89/200\n",
      "Nitin Face X Assignment Last Date 24-feb-2019 new lr:1.00e-01\n",
      "352/352 [==============================] - 93s 265ms/step - loss: 0.4490 - accuracy: 0.9378 - val_loss: 0.7486 - val_accuracy: 0.8522\n",
      "Epoch 90/200\n",
      "Nitin Face X Assignment Last Date 24-feb-2019 new lr:1.00e-01\n",
      "352/352 [==============================] - 93s 265ms/step - loss: 0.4580 - accuracy: 0.9332 - val_loss: 0.6888 - val_accuracy: 0.8644\n",
      "Epoch 91/200\n",
      "Nitin Face X Assignment Last Date 24-feb-2019 new lr:1.00e-01\n",
      "352/352 [==============================] - 93s 265ms/step - loss: 0.4525 - accuracy: 0.9356 - val_loss: 0.8543 - val_accuracy: 0.8256\n",
      "Epoch 92/200\n",
      "Nitin Face X Assignment Last Date 24-feb-2019 new lr:1.00e-01\n",
      "352/352 [==============================] - 93s 265ms/step - loss: 0.4529 - accuracy: 0.9359 - val_loss: 0.7802 - val_accuracy: 0.8406\n",
      "Epoch 93/200\n",
      "Nitin Face X Assignment Last Date 24-feb-2019 new lr:1.00e-02\n",
      "352/352 [==============================] - 93s 265ms/step - loss: 0.3874 - accuracy: 0.9594 - val_loss: 0.5249 - val_accuracy: 0.9216\n",
      "Epoch 94/200\n",
      "Nitin Face X Assignment Last Date 24-feb-2019 new lr:1.00e-02\n",
      "352/352 [==============================] - 93s 265ms/step - loss: 0.3402 - accuracy: 0.9746 - val_loss: 0.5195 - val_accuracy: 0.9276\n",
      "Epoch 95/200\n",
      "Nitin Face X Assignment Last Date 24-feb-2019 new lr:1.00e-02\n",
      "352/352 [==============================] - 93s 266ms/step - loss: 0.3213 - accuracy: 0.9806 - val_loss: 0.5216 - val_accuracy: 0.9252\n",
      "Epoch 96/200\n",
      "Nitin Face X Assignment Last Date 24-feb-2019 new lr:1.00e-02\n",
      "352/352 [==============================] - 93s 266ms/step - loss: 0.3127 - accuracy: 0.9816 - val_loss: 0.5153 - val_accuracy: 0.9272\n",
      "Epoch 97/200\n",
      "Nitin Face X Assignment Last Date 24-feb-2019 new lr:1.00e-02\n",
      "352/352 [==============================] - 93s 265ms/step - loss: 0.2998 - accuracy: 0.9852 - val_loss: 0.5267 - val_accuracy: 0.9280\n",
      "Epoch 98/200\n",
      "Nitin Face X Assignment Last Date 24-feb-2019 new lr:1.00e-02\n",
      "352/352 [==============================] - 93s 266ms/step - loss: 0.2897 - accuracy: 0.9876 - val_loss: 0.5345 - val_accuracy: 0.9264\n",
      "Epoch 99/200\n",
      "Nitin Face X Assignment Last Date 24-feb-2019 new lr:1.00e-02\n",
      "352/352 [==============================] - 94s 266ms/step - loss: 0.2852 - accuracy: 0.9880 - val_loss: 0.5426 - val_accuracy: 0.9240\n",
      "Epoch 100/200\n",
      "Nitin Face X Assignment Last Date 24-feb-2019 new lr:1.00e-02\n",
      "352/352 [==============================] - 93s 266ms/step - loss: 0.2789 - accuracy: 0.9888 - val_loss: 0.5266 - val_accuracy: 0.9276\n",
      "Epoch 101/200\n",
      "Nitin Face X Assignment Last Date 24-feb-2019 new lr:1.00e-02\n",
      "352/352 [==============================] - 94s 266ms/step - loss: 0.2711 - accuracy: 0.9904 - val_loss: 0.5328 - val_accuracy: 0.9236\n",
      "Epoch 102/200\n",
      "Nitin Face X Assignment Last Date 24-feb-2019 new lr:1.00e-02\n",
      "352/352 [==============================] - 93s 266ms/step - loss: 0.2670 - accuracy: 0.9904 - val_loss: 0.5323 - val_accuracy: 0.9268\n",
      "Epoch 103/200\n",
      "Nitin Face X Assignment Last Date 24-feb-2019 new lr:1.00e-02\n",
      "352/352 [==============================] - 94s 266ms/step - loss: 0.2599 - accuracy: 0.9919 - val_loss: 0.5378 - val_accuracy: 0.9272\n",
      "Epoch 104/200\n",
      "Nitin Face X Assignment Last Date 24-feb-2019 new lr:1.00e-02\n",
      "352/352 [==============================] - 94s 266ms/step - loss: 0.2571 - accuracy: 0.9916 - val_loss: 0.5459 - val_accuracy: 0.9264\n",
      "Epoch 105/200\n",
      "Nitin Face X Assignment Last Date 24-feb-2019 new lr:1.00e-02\n",
      "352/352 [==============================] - 94s 266ms/step - loss: 0.2501 - accuracy: 0.9932 - val_loss: 0.5476 - val_accuracy: 0.9250\n",
      "Epoch 106/200\n",
      "Nitin Face X Assignment Last Date 24-feb-2019 new lr:1.00e-02\n",
      "352/352 [==============================] - 94s 266ms/step - loss: 0.2459 - accuracy: 0.9933 - val_loss: 0.5483 - val_accuracy: 0.9288\n",
      "Epoch 107/200\n",
      "Nitin Face X Assignment Last Date 24-feb-2019 new lr:1.00e-02\n",
      "352/352 [==============================] - 94s 266ms/step - loss: 0.2410 - accuracy: 0.9941 - val_loss: 0.5478 - val_accuracy: 0.9284\n",
      "Epoch 108/200\n",
      "Nitin Face X Assignment Last Date 24-feb-2019 new lr:1.00e-02\n",
      "352/352 [==============================] - 94s 266ms/step - loss: 0.2380 - accuracy: 0.9940 - val_loss: 0.5585 - val_accuracy: 0.9262\n",
      "Epoch 109/200\n",
      "Nitin Face X Assignment Last Date 24-feb-2019 new lr:1.00e-02\n",
      "352/352 [==============================] - 94s 266ms/step - loss: 0.2349 - accuracy: 0.9943 - val_loss: 0.5554 - val_accuracy: 0.9266\n",
      "Epoch 110/200\n",
      "Nitin Face X Assignment Last Date 24-feb-2019 new lr:1.00e-02\n",
      "352/352 [==============================] - 94s 266ms/step - loss: 0.2305 - accuracy: 0.9952 - val_loss: 0.5532 - val_accuracy: 0.9262\n",
      "Epoch 111/200\n",
      "Nitin Face X Assignment Last Date 24-feb-2019 new lr:1.00e-02\n",
      "352/352 [==============================] - 94s 266ms/step - loss: 0.2261 - accuracy: 0.9954 - val_loss: 0.5564 - val_accuracy: 0.9290\n",
      "Epoch 112/200\n",
      "Nitin Face X Assignment Last Date 24-feb-2019 new lr:1.00e-02\n",
      "352/352 [==============================] - 94s 266ms/step - loss: 0.2223 - accuracy: 0.9961 - val_loss: 0.5488 - val_accuracy: 0.9282\n",
      "Epoch 113/200\n",
      "Nitin Face X Assignment Last Date 24-feb-2019 new lr:1.00e-02\n",
      "352/352 [==============================] - 94s 266ms/step - loss: 0.2198 - accuracy: 0.9954 - val_loss: 0.5584 - val_accuracy: 0.9268\n",
      "Epoch 114/200\n",
      "Nitin Face X Assignment Last Date 24-feb-2019 new lr:1.00e-02\n",
      "352/352 [==============================] - 94s 267ms/step - loss: 0.2163 - accuracy: 0.9958 - val_loss: 0.5707 - val_accuracy: 0.9252\n",
      "Epoch 115/200\n",
      "Nitin Face X Assignment Last Date 24-feb-2019 new lr:1.00e-02\n",
      "352/352 [==============================] - 94s 266ms/step - loss: 0.2146 - accuracy: 0.9953 - val_loss: 0.5617 - val_accuracy: 0.9242\n",
      "Epoch 116/200\n",
      "Nitin Face X Assignment Last Date 24-feb-2019 new lr:1.00e-02\n",
      "352/352 [==============================] - 94s 266ms/step - loss: 0.2104 - accuracy: 0.9960 - val_loss: 0.5457 - val_accuracy: 0.9276\n",
      "Epoch 117/200\n",
      "Nitin Face X Assignment Last Date 24-feb-2019 new lr:1.00e-02\n",
      "352/352 [==============================] - 94s 266ms/step - loss: 0.2071 - accuracy: 0.9962 - val_loss: 0.5381 - val_accuracy: 0.9274\n",
      "Epoch 118/200\n",
      "Nitin Face X Assignment Last Date 24-feb-2019 new lr:1.00e-02\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "352/352 [==============================] - 93s 265ms/step - loss: 0.2029 - accuracy: 0.9967 - val_loss: 0.5553 - val_accuracy: 0.9288\n",
      "Epoch 119/200\n",
      "Nitin Face X Assignment Last Date 24-feb-2019 new lr:1.00e-02\n",
      "352/352 [==============================] - 93s 265ms/step - loss: 0.2011 - accuracy: 0.9964 - val_loss: 0.5608 - val_accuracy: 0.9260\n",
      "Epoch 120/200\n",
      "Nitin Face X Assignment Last Date 24-feb-2019 new lr:1.00e-02\n",
      "352/352 [==============================] - 93s 265ms/step - loss: 0.1974 - accuracy: 0.9970 - val_loss: 0.5771 - val_accuracy: 0.9244\n",
      "Epoch 121/200\n",
      "Nitin Face X Assignment Last Date 24-feb-2019 new lr:1.00e-02\n",
      "352/352 [==============================] - 93s 265ms/step - loss: 0.1946 - accuracy: 0.9970 - val_loss: 0.5555 - val_accuracy: 0.9278\n",
      "Epoch 122/200\n",
      "Nitin Face X Assignment Last Date 24-feb-2019 new lr:1.00e-02\n",
      "352/352 [==============================] - 93s 265ms/step - loss: 0.1931 - accuracy: 0.9965 - val_loss: 0.5859 - val_accuracy: 0.9244\n",
      "Epoch 123/200\n",
      "Nitin Face X Assignment Last Date 24-feb-2019 new lr:1.00e-02\n",
      "352/352 [==============================] - 93s 265ms/step - loss: 0.1888 - accuracy: 0.9976 - val_loss: 0.5649 - val_accuracy: 0.9258\n",
      "Epoch 124/200\n",
      "Nitin Face X Assignment Last Date 24-feb-2019 new lr:1.00e-02\n",
      "352/352 [==============================] - 93s 265ms/step - loss: 0.1867 - accuracy: 0.9970 - val_loss: 0.5699 - val_accuracy: 0.9248\n",
      "Epoch 125/200\n",
      "Nitin Face X Assignment Last Date 24-feb-2019 new lr:1.00e-02\n",
      "352/352 [==============================] - 94s 267ms/step - loss: 0.1851 - accuracy: 0.9966 - val_loss: 0.5660 - val_accuracy: 0.9262\n",
      "Epoch 126/200\n",
      "Nitin Face X Assignment Last Date 24-feb-2019 new lr:1.00e-02\n",
      "352/352 [==============================] - 93s 265ms/step - loss: 0.1828 - accuracy: 0.9970 - val_loss: 0.5555 - val_accuracy: 0.9266\n",
      "Epoch 127/200\n",
      "Nitin Face X Assignment Last Date 24-feb-2019 new lr:1.00e-02\n",
      "352/352 [==============================] - 93s 265ms/step - loss: 0.1803 - accuracy: 0.9969 - val_loss: 0.5563 - val_accuracy: 0.9280\n",
      "Epoch 128/200\n",
      "Nitin Face X Assignment Last Date 24-feb-2019 new lr:1.00e-02\n",
      "352/352 [==============================] - 93s 266ms/step - loss: 0.1770 - accuracy: 0.9972 - val_loss: 0.5715 - val_accuracy: 0.9266\n",
      "Epoch 129/200\n",
      "Nitin Face X Assignment Last Date 24-feb-2019 new lr:1.00e-02\n",
      "352/352 [==============================] - 93s 265ms/step - loss: 0.1752 - accuracy: 0.9970 - val_loss: 0.5491 - val_accuracy: 0.9284\n",
      "Epoch 130/200\n",
      "Nitin Face X Assignment Last Date 24-feb-2019 new lr:1.00e-02\n",
      "352/352 [==============================] - 93s 265ms/step - loss: 0.1725 - accuracy: 0.9974 - val_loss: 0.5595 - val_accuracy: 0.9262\n",
      "Epoch 131/200\n",
      "Nitin Face X Assignment Last Date 24-feb-2019 new lr:1.00e-02\n",
      "352/352 [==============================] - 93s 265ms/step - loss: 0.1714 - accuracy: 0.9969 - val_loss: 0.5851 - val_accuracy: 0.9260\n",
      "Epoch 132/200\n",
      "Nitin Face X Assignment Last Date 24-feb-2019 new lr:1.00e-02\n",
      "352/352 [==============================] - 93s 265ms/step - loss: 0.1684 - accuracy: 0.9972 - val_loss: 0.5662 - val_accuracy: 0.9248\n",
      "Epoch 133/200\n",
      "Nitin Face X Assignment Last Date 24-feb-2019 new lr:1.00e-02\n",
      "352/352 [==============================] - 93s 265ms/step - loss: 0.1660 - accuracy: 0.9973 - val_loss: 0.5699 - val_accuracy: 0.9244\n",
      "Epoch 134/200\n",
      "Nitin Face X Assignment Last Date 24-feb-2019 new lr:1.00e-02\n",
      "352/352 [==============================] - 94s 266ms/step - loss: 0.1635 - accuracy: 0.9976 - val_loss: 0.5615 - val_accuracy: 0.9288\n",
      "Epoch 135/200\n",
      "Nitin Face X Assignment Last Date 24-feb-2019 new lr:1.00e-02\n",
      "352/352 [==============================] - 94s 266ms/step - loss: 0.1624 - accuracy: 0.9973 - val_loss: 0.5617 - val_accuracy: 0.9262\n",
      "Epoch 136/200\n",
      "Nitin Face X Assignment Last Date 24-feb-2019 new lr:1.00e-02\n",
      "352/352 [==============================] - 93s 265ms/step - loss: 0.1606 - accuracy: 0.9973 - val_loss: 0.5580 - val_accuracy: 0.9292\n",
      "Epoch 137/200\n",
      "Nitin Face X Assignment Last Date 24-feb-2019 new lr:1.00e-02\n",
      "352/352 [==============================] - 94s 266ms/step - loss: 0.1591 - accuracy: 0.9972 - val_loss: 0.5506 - val_accuracy: 0.9304\n",
      "Epoch 138/200\n",
      "Nitin Face X Assignment Last Date 24-feb-2019 new lr:1.00e-02\n",
      "352/352 [==============================] - 94s 266ms/step - loss: 0.1566 - accuracy: 0.9974 - val_loss: 0.5448 - val_accuracy: 0.9274\n",
      "Epoch 139/200\n",
      "Nitin Face X Assignment Last Date 24-feb-2019 new lr:1.00e-03\n",
      "352/352 [==============================] - 94s 266ms/step - loss: 0.1544 - accuracy: 0.9978 - val_loss: 0.5406 - val_accuracy: 0.9292\n",
      "Epoch 140/200\n",
      "Nitin Face X Assignment Last Date 24-feb-2019 new lr:1.00e-03\n",
      "352/352 [==============================] - 94s 266ms/step - loss: 0.1527 - accuracy: 0.9983 - val_loss: 0.5393 - val_accuracy: 0.9284\n",
      "Epoch 141/200\n",
      "Nitin Face X Assignment Last Date 24-feb-2019 new lr:1.00e-03\n",
      "352/352 [==============================] - 94s 266ms/step - loss: 0.1518 - accuracy: 0.9986 - val_loss: 0.5420 - val_accuracy: 0.9278\n",
      "Epoch 142/200\n",
      "Nitin Face X Assignment Last Date 24-feb-2019 new lr:1.00e-03\n",
      "352/352 [==============================] - 94s 266ms/step - loss: 0.1511 - accuracy: 0.9988 - val_loss: 0.5395 - val_accuracy: 0.9284\n",
      "Epoch 143/200\n",
      "Nitin Face X Assignment Last Date 24-feb-2019 new lr:1.00e-03\n",
      "352/352 [==============================] - 94s 266ms/step - loss: 0.1508 - accuracy: 0.9988 - val_loss: 0.5372 - val_accuracy: 0.9280\n",
      "Epoch 144/200\n",
      "Nitin Face X Assignment Last Date 24-feb-2019 new lr:1.00e-03\n",
      "352/352 [==============================] - 94s 266ms/step - loss: 0.1506 - accuracy: 0.9988 - val_loss: 0.5334 - val_accuracy: 0.9296\n",
      "Epoch 145/200\n",
      "Nitin Face X Assignment Last Date 24-feb-2019 new lr:1.00e-03\n",
      "352/352 [==============================] - 94s 266ms/step - loss: 0.1503 - accuracy: 0.9986 - val_loss: 0.5377 - val_accuracy: 0.9280\n",
      "Epoch 146/200\n",
      "Nitin Face X Assignment Last Date 24-feb-2019 new lr:1.00e-03\n",
      "352/352 [==============================] - 94s 266ms/step - loss: 0.1493 - accuracy: 0.9990 - val_loss: 0.5335 - val_accuracy: 0.9286\n",
      "Epoch 147/200\n",
      "Nitin Face X Assignment Last Date 24-feb-2019 new lr:1.00e-03\n",
      "352/352 [==============================] - 94s 266ms/step - loss: 0.1496 - accuracy: 0.9987 - val_loss: 0.5370 - val_accuracy: 0.9284\n",
      "Epoch 148/200\n",
      "Nitin Face X Assignment Last Date 24-feb-2019 new lr:1.00e-03\n",
      "352/352 [==============================] - 94s 266ms/step - loss: 0.1487 - accuracy: 0.9992 - val_loss: 0.5330 - val_accuracy: 0.9308\n",
      "Epoch 149/200\n",
      "Nitin Face X Assignment Last Date 24-feb-2019 new lr:1.00e-03\n",
      "352/352 [==============================] - 94s 266ms/step - loss: 0.1486 - accuracy: 0.9990 - val_loss: 0.5340 - val_accuracy: 0.9298\n",
      "Epoch 150/200\n",
      "Nitin Face X Assignment Last Date 24-feb-2019 new lr:1.00e-03\n",
      "352/352 [==============================] - 94s 266ms/step - loss: 0.1483 - accuracy: 0.9991 - val_loss: 0.5335 - val_accuracy: 0.9302\n",
      "Epoch 151/200\n",
      "Nitin Face X Assignment Last Date 24-feb-2019 new lr:1.00e-03\n",
      "352/352 [==============================] - 94s 267ms/step - loss: 0.1485 - accuracy: 0.9990 - val_loss: 0.5321 - val_accuracy: 0.9302\n",
      "Epoch 152/200\n",
      "Nitin Face X Assignment Last Date 24-feb-2019 new lr:1.00e-03\n",
      "352/352 [==============================] - 94s 266ms/step - loss: 0.1477 - accuracy: 0.9990 - val_loss: 0.5323 - val_accuracy: 0.9304\n",
      "Epoch 153/200\n",
      "Nitin Face X Assignment Last Date 24-feb-2019 new lr:1.00e-03\n",
      "352/352 [==============================] - 94s 266ms/step - loss: 0.1482 - accuracy: 0.9989 - val_loss: 0.5373 - val_accuracy: 0.9292\n",
      "Epoch 154/200\n",
      "Nitin Face X Assignment Last Date 24-feb-2019 new lr:1.00e-03\n",
      "352/352 [==============================] - 94s 267ms/step - loss: 0.1470 - accuracy: 0.9993 - val_loss: 0.5352 - val_accuracy: 0.9296\n",
      "Epoch 155/200\n",
      "Nitin Face X Assignment Last Date 24-feb-2019 new lr:1.00e-03\n",
      "352/352 [==============================] - 94s 266ms/step - loss: 0.1473 - accuracy: 0.9992 - val_loss: 0.5353 - val_accuracy: 0.9286\n",
      "Epoch 156/200\n",
      "Nitin Face X Assignment Last Date 24-feb-2019 new lr:1.00e-03\n",
      "352/352 [==============================] - 94s 266ms/step - loss: 0.1474 - accuracy: 0.9990 - val_loss: 0.5354 - val_accuracy: 0.9292\n",
      "Epoch 157/200\n",
      "Nitin Face X Assignment Last Date 24-feb-2019 new lr:1.00e-03\n",
      "352/352 [==============================] - 93s 265ms/step - loss: 0.1465 - accuracy: 0.9992 - val_loss: 0.5391 - val_accuracy: 0.9294\n",
      "Epoch 158/200\n",
      "Nitin Face X Assignment Last Date 24-feb-2019 new lr:1.00e-03\n",
      "352/352 [==============================] - 93s 265ms/step - loss: 0.1464 - accuracy: 0.9992 - val_loss: 0.5376 - val_accuracy: 0.9290\n",
      "Epoch 159/200\n",
      "Nitin Face X Assignment Last Date 24-feb-2019 new lr:1.00e-03\n",
      "352/352 [==============================] - 93s 265ms/step - loss: 0.1462 - accuracy: 0.9992 - val_loss: 0.5394 - val_accuracy: 0.9286\n",
      "Epoch 160/200\n",
      "Nitin Face X Assignment Last Date 24-feb-2019 new lr:1.00e-03\n",
      "352/352 [==============================] - 93s 265ms/step - loss: 0.1459 - accuracy: 0.9993 - val_loss: 0.5404 - val_accuracy: 0.9290\n",
      "Epoch 161/200\n",
      "Nitin Face X Assignment Last Date 24-feb-2019 new lr:1.00e-03\n",
      "352/352 [==============================] - 93s 265ms/step - loss: 0.1455 - accuracy: 0.9993 - val_loss: 0.5392 - val_accuracy: 0.9270\n",
      "Epoch 162/200\n",
      "Nitin Face X Assignment Last Date 24-feb-2019 new lr:1.00e-03\n",
      "352/352 [==============================] - 93s 265ms/step - loss: 0.1453 - accuracy: 0.9994 - val_loss: 0.5376 - val_accuracy: 0.9288\n",
      "Epoch 163/200\n",
      "Nitin Face X Assignment Last Date 24-feb-2019 new lr:1.00e-03\n",
      "352/352 [==============================] - 93s 265ms/step - loss: 0.1453 - accuracy: 0.9993 - val_loss: 0.5435 - val_accuracy: 0.9300\n",
      "Epoch 164/200\n",
      "Nitin Face X Assignment Last Date 24-feb-2019 new lr:1.00e-03\n",
      "352/352 [==============================] - 93s 265ms/step - loss: 0.1450 - accuracy: 0.9992 - val_loss: 0.5380 - val_accuracy: 0.9296\n",
      "Epoch 165/200\n",
      "Nitin Face X Assignment Last Date 24-feb-2019 new lr:1.00e-03\n",
      "352/352 [==============================] - 93s 265ms/step - loss: 0.1444 - accuracy: 0.9994 - val_loss: 0.5390 - val_accuracy: 0.9296\n",
      "Epoch 166/200\n",
      "Nitin Face X Assignment Last Date 24-feb-2019 new lr:1.00e-03\n",
      "352/352 [==============================] - 94s 266ms/step - loss: 0.1447 - accuracy: 0.9993 - val_loss: 0.5433 - val_accuracy: 0.9292\n",
      "Epoch 167/200\n",
      "Nitin Face X Assignment Last Date 24-feb-2019 new lr:1.00e-03\n",
      "352/352 [==============================] - 93s 266ms/step - loss: 0.1442 - accuracy: 0.9994 - val_loss: 0.5422 - val_accuracy: 0.9290\n",
      "Epoch 168/200\n",
      "Nitin Face X Assignment Last Date 24-feb-2019 new lr:1.00e-03\n",
      "352/352 [==============================] - 93s 265ms/step - loss: 0.1437 - accuracy: 0.9993 - val_loss: 0.5410 - val_accuracy: 0.9298\n",
      "Epoch 169/200\n",
      "Nitin Face X Assignment Last Date 24-feb-2019 new lr:1.00e-03\n",
      "352/352 [==============================] - 93s 265ms/step - loss: 0.1434 - accuracy: 0.9995 - val_loss: 0.5432 - val_accuracy: 0.9288\n",
      "Epoch 170/200\n",
      "Nitin Face X Assignment Last Date 24-feb-2019 new lr:1.00e-03\n",
      "352/352 [==============================] - 93s 266ms/step - loss: 0.1432 - accuracy: 0.9994 - val_loss: 0.5434 - val_accuracy: 0.9290\n",
      "Epoch 171/200\n",
      "Nitin Face X Assignment Last Date 24-feb-2019 new lr:1.00e-03\n",
      "352/352 [==============================] - 93s 266ms/step - loss: 0.1436 - accuracy: 0.9992 - val_loss: 0.5439 - val_accuracy: 0.9288\n",
      "Epoch 172/200\n",
      "Nitin Face X Assignment Last Date 24-feb-2019 new lr:1.00e-03\n",
      "352/352 [==============================] - 93s 266ms/step - loss: 0.1430 - accuracy: 0.9995 - val_loss: 0.5451 - val_accuracy: 0.9290\n",
      "Epoch 173/200\n",
      "Nitin Face X Assignment Last Date 24-feb-2019 new lr:1.00e-03\n",
      "352/352 [==============================] - 94s 266ms/step - loss: 0.1431 - accuracy: 0.9991 - val_loss: 0.5430 - val_accuracy: 0.9282\n",
      "Epoch 174/200\n",
      "Nitin Face X Assignment Last Date 24-feb-2019 new lr:1.00e-03\n",
      "352/352 [==============================] - 93s 266ms/step - loss: 0.1425 - accuracy: 0.9994 - val_loss: 0.5425 - val_accuracy: 0.9286\n",
      "Epoch 175/200\n",
      "Nitin Face X Assignment Last Date 24-feb-2019 new lr:1.00e-03\n",
      "352/352 [==============================] - 94s 266ms/step - loss: 0.1420 - accuracy: 0.9996 - val_loss: 0.5442 - val_accuracy: 0.9294\n",
      "Epoch 176/200\n",
      "Nitin Face X Assignment Last Date 24-feb-2019 new lr:1.00e-03\n",
      "352/352 [==============================] - 94s 266ms/step - loss: 0.1421 - accuracy: 0.9995 - val_loss: 0.5425 - val_accuracy: 0.9300\n",
      "Epoch 177/200\n",
      "Nitin Face X Assignment Last Date 24-feb-2019 new lr:1.00e-03\n",
      "352/352 [==============================] - 94s 266ms/step - loss: 0.1415 - accuracy: 0.9997 - val_loss: 0.5440 - val_accuracy: 0.9302\n",
      "Epoch 178/200\n",
      "Nitin Face X Assignment Last Date 24-feb-2019 new lr:1.00e-03\n",
      "352/352 [==============================] - 94s 266ms/step - loss: 0.1415 - accuracy: 0.9996 - val_loss: 0.5443 - val_accuracy: 0.9292\n",
      "Epoch 179/200\n",
      "Nitin Face X Assignment Last Date 24-feb-2019 new lr:1.00e-03\n",
      "352/352 [==============================] - 94s 266ms/step - loss: 0.1414 - accuracy: 0.9994 - val_loss: 0.5439 - val_accuracy: 0.9300\n",
      "Epoch 180/200\n",
      "Nitin Face X Assignment Last Date 24-feb-2019 new lr:1.00e-03\n",
      "352/352 [==============================] - 94s 266ms/step - loss: 0.1411 - accuracy: 0.9995 - val_loss: 0.5428 - val_accuracy: 0.9312\n",
      "Epoch 181/200\n",
      "Nitin Face X Assignment Last Date 24-feb-2019 new lr:1.00e-03\n",
      "352/352 [==============================] - 94s 266ms/step - loss: 0.1409 - accuracy: 0.9993 - val_loss: 0.5437 - val_accuracy: 0.9294\n",
      "Epoch 182/200\n",
      "Nitin Face X Assignment Last Date 24-feb-2019 new lr:1.00e-03\n",
      "352/352 [==============================] - 94s 266ms/step - loss: 0.1408 - accuracy: 0.9994 - val_loss: 0.5441 - val_accuracy: 0.9316\n",
      "Epoch 183/200\n",
      "Nitin Face X Assignment Last Date 24-feb-2019 new lr:1.00e-03\n",
      "352/352 [==============================] - 94s 267ms/step - loss: 0.1407 - accuracy: 0.9994 - val_loss: 0.5454 - val_accuracy: 0.9312\n",
      "Epoch 184/200\n",
      "Nitin Face X Assignment Last Date 24-feb-2019 new lr:1.00e-03\n",
      "352/352 [==============================] - 94s 266ms/step - loss: 0.1403 - accuracy: 0.9994 - val_loss: 0.5451 - val_accuracy: 0.9304\n",
      "Epoch 185/200\n",
      "Nitin Face X Assignment Last Date 24-feb-2019 new lr:1.00e-03\n",
      "352/352 [==============================] - 94s 266ms/step - loss: 0.1401 - accuracy: 0.9994 - val_loss: 0.5460 - val_accuracy: 0.9298\n",
      "Epoch 186/200\n",
      "Nitin Face X Assignment Last Date 24-feb-2019 new lr:1.00e-03\n",
      "352/352 [==============================] - 94s 266ms/step - loss: 0.1400 - accuracy: 0.9995 - val_loss: 0.5471 - val_accuracy: 0.9294\n",
      "Epoch 187/200\n",
      "Nitin Face X Assignment Last Date 24-feb-2019 new lr:1.00e-03\n",
      "352/352 [==============================] - 94s 266ms/step - loss: 0.1397 - accuracy: 0.9994 - val_loss: 0.5451 - val_accuracy: 0.9294\n",
      "Epoch 188/200\n",
      "Nitin Face X Assignment Last Date 24-feb-2019 new lr:1.00e-03\n",
      "352/352 [==============================] - 94s 266ms/step - loss: 0.1394 - accuracy: 0.9995 - val_loss: 0.5458 - val_accuracy: 0.9308\n",
      "Epoch 189/200\n",
      "Nitin Face X Assignment Last Date 24-feb-2019 new lr:1.00e-03\n",
      "352/352 [==============================] - 94s 266ms/step - loss: 0.1392 - accuracy: 0.9995 - val_loss: 0.5465 - val_accuracy: 0.9316\n",
      "Epoch 190/200\n",
      "Nitin Face X Assignment Last Date 24-feb-2019 new lr:1.00e-03\n",
      "352/352 [==============================] - 94s 267ms/step - loss: 0.1391 - accuracy: 0.9996 - val_loss: 0.5458 - val_accuracy: 0.9292\n",
      "Epoch 191/200\n",
      "Nitin Face X Assignment Last Date 24-feb-2019 new lr:1.00e-03\n",
      "352/352 [==============================] - 94s 266ms/step - loss: 0.1386 - accuracy: 0.9997 - val_loss: 0.5449 - val_accuracy: 0.9294\n",
      "Epoch 192/200\n",
      "Nitin Face X Assignment Last Date 24-feb-2019 new lr:1.00e-03\n",
      "352/352 [==============================] - 94s 266ms/step - loss: 0.1388 - accuracy: 0.9995 - val_loss: 0.5471 - val_accuracy: 0.9296\n",
      "Epoch 193/200\n",
      "Nitin Face X Assignment Last Date 24-feb-2019 new lr:1.00e-03\n",
      "352/352 [==============================] - 94s 266ms/step - loss: 0.1388 - accuracy: 0.9994 - val_loss: 0.5460 - val_accuracy: 0.9298\n",
      "Epoch 194/200\n",
      "Nitin Face X Assignment Last Date 24-feb-2019 new lr:1.00e-03\n",
      "352/352 [==============================] - 94s 267ms/step - loss: 0.1382 - accuracy: 0.9994 - val_loss: 0.5447 - val_accuracy: 0.9286\n",
      "Epoch 195/200\n",
      "Nitin Face X Assignment Last Date 24-feb-2019 new lr:1.00e-03\n",
      "352/352 [==============================] - 94s 266ms/step - loss: 0.1381 - accuracy: 0.9995 - val_loss: 0.5468 - val_accuracy: 0.9308\n",
      "Epoch 196/200\n",
      "Nitin Face X Assignment Last Date 24-feb-2019 new lr:1.00e-03\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "352/352 [==============================] - 93s 265ms/step - loss: 0.1378 - accuracy: 0.9995 - val_loss: 0.5443 - val_accuracy: 0.9306\n",
      "Epoch 197/200\n",
      "Nitin Face X Assignment Last Date 24-feb-2019 new lr:1.00e-03\n",
      "352/352 [==============================] - 93s 265ms/step - loss: 0.1376 - accuracy: 0.9995 - val_loss: 0.5445 - val_accuracy: 0.9306\n",
      "Epoch 198/200\n",
      "Nitin Face X Assignment Last Date 24-feb-2019 new lr:1.00e-03\n",
      "352/352 [==============================] - 93s 265ms/step - loss: 0.1376 - accuracy: 0.9995 - val_loss: 0.5457 - val_accuracy: 0.9314\n",
      "Epoch 199/200\n",
      "Nitin Face X Assignment Last Date 24-feb-2019 new lr:1.00e-03\n",
      "352/352 [==============================] - 93s 265ms/step - loss: 0.1373 - accuracy: 0.9995 - val_loss: 0.5477 - val_accuracy: 0.9316\n",
      "Epoch 200/200\n",
      "Nitin Face X Assignment Last Date 24-feb-2019 new lr:1.00e-03\n",
      "352/352 [==============================] - 93s 265ms/step - loss: 0.1374 - accuracy: 0.9995 - val_loss: 0.5471 - val_accuracy: 0.9312\n",
      "CPU times: user 6h 43min 55s, sys: 35min 13s, total: 7h 19min 8s\n",
      "Wall time: 5h 12min 46s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from cifar10_solver import *\n",
    "\n",
    "from keras.callbacks import LearningRateScheduler\n",
    "\n",
    "def lr_scheduler(epoch):\n",
    "    new_lr = lr\n",
    "    if epoch <= 91:\n",
    "        pass\n",
    "    elif epoch > 91 and epoch <= 137:\n",
    "        new_lr = lr * 0.1\n",
    "    else:\n",
    "        new_lr = lr * 0.01\n",
    "    print('Nitin Face X Assignment Last Date 24-feb-2019 new lr:%.2e' % new_lr)\n",
    "    return new_lr \n",
    "\n",
    "\n",
    "reduce_lr = LearningRateScheduler(lr_scheduler)\n",
    "\n",
    "solver = CIFAR10Solver(resnet56, data)\n",
    "history = solver.train(epochs=200, batch_size=128, data_augmentation=True, callbacks=[reduce_lr])\n",
    "\n",
    "resnet56.save(\"RESNET.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 8s 831us/step\n",
      "test data loss:0.53 acc:0.9283\n"
     ]
    }
   ],
   "source": [
    "solver.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
